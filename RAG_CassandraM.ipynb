{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "feacf9d02531409ba7eca34098405682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e54271a4198f4d2992d56f83c58f6dea",
              "IPY_MODEL_a3178852ed124aafb01a6becfc21bff8",
              "IPY_MODEL_3dd860a332ab44119e8b78930e4a6a29"
            ],
            "layout": "IPY_MODEL_b8c609ccfd4a491baa467aea7d0d6d6c"
          }
        },
        "e54271a4198f4d2992d56f83c58f6dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f540f67562a846bea04dc1ff4c7a8846",
            "placeholder": "​",
            "style": "IPY_MODEL_8eb8c200cd884f1d8c0c895e54fdb5bf",
            "value": "Batches: 100%"
          }
        },
        "a3178852ed124aafb01a6becfc21bff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124f8a30c10c4623a40de1418cf13e77",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b7aed4972854f5fbecf2413bf300d5e",
            "value": 2
          }
        },
        "3dd860a332ab44119e8b78930e4a6a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d91ba16a96a41edadfc43610738399b",
            "placeholder": "​",
            "style": "IPY_MODEL_8327f6442ddd4bd28ab08998f8947d93",
            "value": " 2/2 [00:09&lt;00:00,  4.62s/it]"
          }
        },
        "b8c609ccfd4a491baa467aea7d0d6d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f540f67562a846bea04dc1ff4c7a8846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb8c200cd884f1d8c0c895e54fdb5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124f8a30c10c4623a40de1418cf13e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7aed4972854f5fbecf2413bf300d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d91ba16a96a41edadfc43610738399b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8327f6442ddd4bd28ab08998f8947d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1c7ca5ab0994d339e36b820e4958de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7dca4810090424586f78562d92b830e",
              "IPY_MODEL_7ac809ad0c0a4850a5002a3a89c65f98",
              "IPY_MODEL_67248c8bbf834240b6820e86fb78fa3c"
            ],
            "layout": "IPY_MODEL_3d4d0cdf9f9045eeb9dd072f76ba1c46"
          }
        },
        "e7dca4810090424586f78562d92b830e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7424d5d187324be0bc35367bb0841516",
            "placeholder": "​",
            "style": "IPY_MODEL_b939b9b9266942a39e016244ba0cef05",
            "value": "Batches: 100%"
          }
        },
        "7ac809ad0c0a4850a5002a3a89c65f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b36c193ffe2e4193b88a7397a8020c2d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90e99468dd6142b8b010ce9ff500149a",
            "value": 2
          }
        },
        "67248c8bbf834240b6820e86fb78fa3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0df985cdf7de457198f4e8b57e50382f",
            "placeholder": "​",
            "style": "IPY_MODEL_63f0254966ad41ee893c73cd3de05016",
            "value": " 2/2 [00:10&lt;00:00,  4.92s/it]"
          }
        },
        "3d4d0cdf9f9045eeb9dd072f76ba1c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7424d5d187324be0bc35367bb0841516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b939b9b9266942a39e016244ba0cef05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b36c193ffe2e4193b88a7397a8020c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e99468dd6142b8b010ce9ff500149a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0df985cdf7de457198f4e8b57e50382f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63f0254966ad41ee893c73cd3de05016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu PyPDF2 torch openai python-dotenv\n",
        "!pip install --upgrade langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcS2HsGEkT4o",
        "outputId": "749c1c5d-366e-4da9-e552-494041664a41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4D6yNVDt0fSm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My function is handling the PDF and the text extraction.\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.documents = {}\n",
        "\n",
        "    # Extracting the text from the file.\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    page_text = page.extract_text()\n",
        "                    # Table detection and formatting.\n",
        "                    page_text = self.enhance_table_extraction(page_text)\n",
        "                    text += f\"\\Page {page_num + 1} {page_text}.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "        return text\n",
        "\n",
        "# Table formatting.\n",
        "    def enhance_table_extraction(self, text: str) -> str:\n",
        "        \"\"\"Aggressively improve table formatting and preserve critical academic content\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        processed_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Self-route method.\n",
        "            if re.search(r'(self-route|routing|self-reflection)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"SELF_ROUTE_CONTENT: {line}\")\n",
        "\n",
        "            # Failure type lists.\n",
        "            elif re.search(r'(failure|error).*(type|case|category)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"FAILURE_TYPES: {line}\")\n",
        "            elif re.search(r'(multi-step|general knowledge|implicit|long.?complex)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"FAILURE_DETAIL: {line}\")\n",
        "\n",
        "            # Evaluation metrics and tables.\n",
        "            elif re.search(r'(mrr|recall@|ndcg@|precision|f1)', line, re.IGNORECASE):\n",
        "                # Keeping the table structure with space.\n",
        "                line = re.sub(r'\\s+', ' | ', line)\n",
        "                processed_lines.append(f\"METRICS_TABLE: {line}\")\n",
        "\n",
        "            # Chunking strategy content.\n",
        "            elif re.search(r'(chunk|segment|overlap|window)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"CHUNKING_STRATEGY: {line}\")\n",
        "\n",
        "            # Performance comparisons.\n",
        "            elif re.search(r'(outperform|superior|better|vs|versus|comparison)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"PERFORMANCE_COMPARISON: {line}\")\n",
        "\n",
        "            # Method objectives and goals.\n",
        "            elif re.search(r'(goal|objective|aim|purpose|method)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"METHOD_GOAL: {line}\")\n",
        "\n",
        "            else:\n",
        "                processed_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(processed_lines)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "        # Preserve important academic patterns\n",
        "        text = re.sub(r'([.!?])\\s+([A-Z])', r'\\1\\n\\2', text)  # Sentence boundaries\n",
        "\n",
        "        # Keep important punctuation and academic notation\n",
        "        text = re.sub(r'[^\\w\\s.,;:!?()%@\\-\\[\\]{}|]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "# Cleaning the text by removing extra whitespaces, special characters while keeping punctations.\n",
        "    def upload_and_process_pdfs(self) -> Dict[str, str]:\n",
        "        print(\"Upload the PDF file.\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        for filename, content in uploaded.items():\n",
        "            if filename.endswith('.pdf'):\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                text = self.extract_text_from_pdf(filename)\n",
        "                cleaned_text = self.clean_text(text)\n",
        "                self.documents[filename] = cleaned_text\n",
        "                print(f\"Processed {filename}: {len(cleaned_text)} characters\")\n",
        "\n",
        "        return self.documents"
      ],
      "metadata": {
        "id": "wkZMFJwaWIn7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the text chunking with overlap.\n",
        "class TextChunker:\n",
        "    def __init__(self, chunk_size: int = 512, overlap: int = 100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "# Splitting by paragraphs first, then by sentences for better coherence.\n",
        "    def chunk_text(self, text: str, document_name: str) -> List[Dict]:\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        chunks = []\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        word_count = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_words = sentence.split()\n",
        "\n",
        "                if word_count + len(sentence_words) > self.chunk_size and current_chunk:\n",
        "                    chunks.append({\n",
        "                        'text': current_chunk.strip(),\n",
        "                        'document': document_name,\n",
        "                        'chunk_id': len(chunks),\n",
        "                        'word_count': word_count\n",
        "                    })\n",
        "\n",
        "       # New chunk with overlap.\n",
        "                    overlap_text = ' '.join(current_chunk.split()[-self.overlap:])\n",
        "                    current_chunk = overlap_text + \" \" + sentence\n",
        "                    word_count = len(current_chunk.split())\n",
        "                else:\n",
        "                    current_chunk += \" \" + sentence\n",
        "                    word_count += len(sentence_words)\n",
        "\n",
        "        # Final chunk.\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'text': current_chunk.strip(),\n",
        "                'document': document_name,\n",
        "                'chunk_id': len(chunks),\n",
        "                'word_count': word_count\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_documents(self, documents: Dict[str, str]) -> List[Dict]:\n",
        "        all_chunks = []\n",
        "        for doc_name, text in documents.items():\n",
        "            chunks = self.chunk_text(text, doc_name)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"Created {len(all_chunks)} chunks total\")\n",
        "        return all_chunks"
      ],
      "metadata": {
        "id": "JWC0gQFUYh2V"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document embeddings and retrieving.\n",
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.embeddings = None\n",
        "        self.chunks = None\n",
        "        self.index = None\n",
        "\n",
        "    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:\n",
        "        texts = [chunk['text'] for chunk in chunks]\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        self.chunks = chunks\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        self.index.add(embeddings_normalized.astype('float32'))\n",
        "\n",
        "        print(f\"Created {len(embeddings)} embeddings with dimension {dimension}\")\n",
        "        return embeddings\n",
        "\n",
        "# For targeted query expansion to find more specific content.\n",
        "    def expand_query(self, query: str) -> List[str]:\n",
        "        base_query = query.lower()\n",
        "        expanded_queries = [query]\n",
        "\n",
        "        # Specific targeting.\n",
        "        if 'self-route' in base_query and 'goal' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"SELF-ROUTE self-reflection routing decision\",\n",
        "                \"model self-reflection dynamically route queries\",\n",
        "                \"routing between RAG and LC cost context length\",\n",
        "                \"Zhuowan Li SELF-ROUTE method objective\",\n",
        "                \"self-reflection mechanism route queries RAG long-context\"\n",
        "            ])\n",
        "\n",
        "        # Failure types.\n",
        "        if 'failure' in base_query and ('four' in base_query or 'types' in base_query or 'cases' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"four failure types RAG multi-step general implicit long complex\",\n",
        "                \"Multi-step reasoning failure General knowledge failure\",\n",
        "                \"Implicit knowledge failure Long complex context failure\",\n",
        "                \"failure categories RAG handling long context\",\n",
        "                \"Zhuowan Li four key failure cases\"\n",
        "            ])\n",
        "\n",
        "        # Targeting Wang's paper.\n",
        "        if 'chunking' in base_query and 'trade-off' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"chunking strategies overlap non-overlap performance cost\",\n",
        "                \"Wang paper chunking module section\",\n",
        "                \"chunk size overlap impact retrieval performance\",\n",
        "                \"segmentation strategies recall faithfulness trade-offs\",\n",
        "                \"chunking overlap computational efficiency\"\n",
        "            ])\n",
        "\n",
        "        # Targeting Wang's evaluation section.\n",
        "        if 'embedding' in base_query and ('metric' in base_query or 'evaluate' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"Wang embedding models MRR Recall@5 Recall@10 nDCG@10\",\n",
        "                \"BGE LLM-Embedder evaluation metrics comparison\",\n",
        "                \"embedding model performance evaluation Wang\",\n",
        "                \"retrieval evaluation metrics MRR nDCG recall\",\n",
        "                \"Wang paper embedding evaluation results\"\n",
        "            ])\n",
        "\n",
        "        # Reranking.\n",
        "        if 'reranking' in base_query or 'rerank' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"Wang reranking techniques retrieval quality impact\",\n",
        "                \"reranking methods comparison Wang paper\",\n",
        "                \"retrieval reranking performance improvement\"\n",
        "            ])\n",
        "\n",
        "\n",
        "        if ('rag' in base_query and 'useful' in base_query) or ('despite' in base_query and 'superior' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"RAG cheaper cost efficient overlap 63% LC models\",\n",
        "                \"RAG benefits despite long-context LLM superiority\",\n",
        "                \"cost efficiency RAG vs long-context LLMs\",\n",
        "                \"why RAG still useful cheaper computational cost\"\n",
        "            ])\n",
        "\n",
        "        # LC vs RAG performance.\n",
        "        if 'long-context' in base_query and ('outperform' in base_query or 'superior' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"long-context LLMs outperformed RAG complex reasoning\",\n",
        "                \"Zhuowan LC LLM superior performance RAG efficiency\",\n",
        "                \"long-context models better complex multi-document queries\"\n",
        "            ])\n",
        "\n",
        "        # Multimodal capabilities.\n",
        "        if 'multimodal' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"multimodal retrieval cross-modal capabilities enhancement\",\n",
        "                \"text image multimodal search RAG\",\n",
        "                \"multimodal RAG vision language models\"\n",
        "            ])\n",
        "\n",
        "        # Rewriting queries.\n",
        "        if 'query rewriting' in base_query or 'query enhancement' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"Wang query rewriting efficiency findings\",\n",
        "                \"query enhancement reformulation techniques Wang\",\n",
        "                \"query rewriting impact RAG efficiency\"\n",
        "            ])\n",
        "\n",
        "        # Self-reflection.\n",
        "        if 'implications' in base_query and 'self-reflection' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"self-reflection routing implications consequences\",\n",
        "                \"adaptive routing decision impact analysis\",\n",
        "                \"SELF-ROUTE self-reflection routing benefits\"\n",
        "            ])\n",
        "\n",
        "        return expanded_queries\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query: str, top_k: int = 8) -> List[Dict]:  # Increased top_k\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not created\")\n",
        "\n",
        "        # Query expansion.\n",
        "        expanded_queries = self.expand_query(query)\n",
        "        all_results = []\n",
        "\n",
        "        for exp_query in expanded_queries:\n",
        "            query_embedding = self.model.encode([exp_query])\n",
        "            query_normalized = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "\n",
        "            scores, indices = self.index.search(query_normalized.astype('float32'), top_k)\n",
        "\n",
        "            for score, idx in zip(scores[0], indices[0]):\n",
        "                chunk = self.chunks[idx].copy()\n",
        "                chunk['similarity_score'] = float(score)\n",
        "                chunk['query_variant'] = exp_query\n",
        "                all_results.append(chunk)\n",
        "\n",
        "        # Removing duplicates and sorting by score.\n",
        "        seen_chunks = set()\n",
        "        unique_results = []\n",
        "        for result in all_results:\n",
        "            chunk_id = result['chunk_id']\n",
        "            if chunk_id not in seen_chunks:\n",
        "                seen_chunks.add(chunk_id)\n",
        "                unique_results.append(result)\n",
        "\n",
        "        # Sortinf by the similarity score.\n",
        "        unique_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
        "        return unique_results[:top_k]"
      ],
      "metadata": {
        "id": "l8N-at3WY1Rz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG system.\n",
        "class ImprovedRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "        self.generator = None\n",
        "        self.setup_generator()\n",
        "\n",
        "    def setup_generator(self):\n",
        "        try:\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=\"distilgpt2\",\n",
        "                tokenizer=\"distilgpt2\",\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                return_full_text=False,\n",
        "                max_new_tokens=150,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            print(\"Generator loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading generator: {e}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def setup_documents(self, documents: Dict[str, str]):\n",
        "        chunker = TextChunker(chunk_size=400, overlap=80)\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "        self.embedding_manager.create_embeddings(chunks)\n",
        "        print(\"RAG system ready.\")\n",
        "\n",
        "# Focused context based on the query type.\n",
        "    def create_focused_context(self, chunks: List[Dict], query: str, max_length: int = 1000) -> str:\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "\n",
        "        # Prioritizing chunks with higher similarity scores.\n",
        "        sorted_chunks = sorted(chunks, key=lambda x: x['similarity_score'], reverse=True)\n",
        "\n",
        "        for i, chunk in enumerate(sorted_chunks):\n",
        "            # Extracting the most relevant sentences from each chunk.\n",
        "            relevant_sentences = self.extract_relevant_sentences(chunk['text'], query)\n",
        "\n",
        "            if relevant_sentences:\n",
        "                chunk_context = f\"[Source {i+1}]: {relevant_sentences}\"\n",
        "\n",
        "                if current_length + len(chunk_context) > max_length:\n",
        "                    break\n",
        "\n",
        "                context_parts.append(chunk_context)\n",
        "                current_length += len(chunk_context)\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def extract_relevant_sentences(self, text: str, query: str) -> str:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        scored_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 15:\n",
        "                continue\n",
        "\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            # Calculating the overlap score.\n",
        "            overlap = len(query_words.intersection(sentence_words))\n",
        "            if any(term in sentence.lower() for term in ['rag', 'llm', 'retrieval', 'embedding', 'failure', 'performance']):\n",
        "                overlap += 1\n",
        "\n",
        "            if overlap > 0:\n",
        "                scored_sentences.append((overlap, sentence))\n",
        "\n",
        "        # Sorting by relevance.\n",
        "        scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "        top_sentences = [sent for _, sent in scored_sentences[:3]]\n",
        "\n",
        "        return \". \".join(top_sentences) + \".\" if top_sentences else \"\"\n",
        "\n",
        "    def generate_answer_with_reasoning(self, query: str, context: str) -> str:\n",
        "        \"\"\"Skip generative approach, it's causing hallucinations\"\"\"\n",
        "        return \"\"\n",
        "\n",
        "    def generate_extractive_answer(self, query: str, context: str) -> str:\n",
        "        if not context:\n",
        "            return \"No relevant information found in the documents.\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Specially tagged content first.\n",
        "        tagged_content = self.extract_tagged_content(context, query_lower)\n",
        "        if tagged_content:\n",
        "            return tagged_content\n",
        "\n",
        "        # Fallback to normal extraction.\n",
        "        return self.extract_regular_content(context, query_lower)\n",
        "\n",
        " # Extracting content based on special tags.\n",
        "    def extract_tagged_content(self, context: str, query: str) -> str:\n",
        "        lines = context.split('\\n')\n",
        "        relevant_lines = []\n",
        "\n",
        "        # Self-route targeting.\n",
        "        if 'self-route' in query and 'goal' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('SELF_ROUTE_CONTENT:') or line.startswith('METHOD_GOAL:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    if any(term in content.lower() for term in ['route', 'routing', 'reflection', 'decision']):\n",
        "                        relevant_lines.append(content)\n",
        "\n",
        "        # Failure types targeting.\n",
        "        elif 'failure' in query and ('four' in query or 'types' in query or 'cases' in query):\n",
        "            failure_content = []\n",
        "            for line in lines:\n",
        "                if line.startswith('FAILURE_TYPES:') or line.startswith('FAILURE_DETAIL:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    failure_content.append(content)\n",
        "\n",
        "            if failure_content:\n",
        "                enumerated = []\n",
        "                for content in failure_content:\n",
        "                    if re.search(r'(1\\.|first|multi-step)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"1) Multi-step reasoning failure: {content}\")\n",
        "                    elif re.search(r'(2\\.|second|general)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"2) General knowledge failure: {content}\")\n",
        "                    elif re.search(r'(3\\.|third|implicit)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"3) Implicit knowledge failure: {content}\")\n",
        "                    elif re.search(r'(4\\.|fourth|long|complex)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"4) Long/complex context failure: {content}\")\n",
        "\n",
        "                if enumerated:\n",
        "                    return \". \".join(enumerated) + \".\"\n",
        "                else:\n",
        "                    return \". \".join(failure_content) + \".\"\n",
        "\n",
        "        # Chunking trade-offs targeting.\n",
        "        elif 'chunking' in query and 'trade-off' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('CHUNKING_STRATEGY:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    if any(term in content.lower() for term in ['overlap', 'trade-off', 'balance', 'cost', 'performance']):\n",
        "                        relevant_lines.append(content)\n",
        "\n",
        "        # Metrics targeting.\n",
        "        elif 'metric' in query and 'embedding' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('METRICS_TABLE:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    relevant_lines.append(content)\n",
        "\n",
        "        # Performance comparison targeting.\n",
        "        elif 'outperform' in query or 'superior' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('PERFORMANCE_COMPARISON:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    relevant_lines.append(content)\n",
        "\n",
        "        if relevant_lines:\n",
        "            unique_lines = []\n",
        "            seen = set()\n",
        "            for line in relevant_lines:\n",
        "                if line not in seen:\n",
        "                    unique_lines.append(line)\n",
        "                    seen.add(line)\n",
        "\n",
        "            return \". \".join(unique_lines[:3]) + \".\"\n",
        "\n",
        "        return \"\"\n",
        "\n",
        " # Extracting the content using normal sentence matching.\n",
        "    def extract_regular_content(self, context: str, query: str) -> str:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "        scored_sentences = []\n",
        "\n",
        "        query_words = set(query.split())\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 25:\n",
        "                continue\n",
        "\n",
        "            sentence_lower = sentence.lower()\n",
        "            sentence_words = set(sentence_lower.split())\n",
        "\n",
        "            # Relevance score.\n",
        "            overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "            bonus = 0\n",
        "\n",
        "            # Self-route bonuses.\n",
        "            if 'self-route' in query:\n",
        "                if any(term in sentence_lower for term in ['self-route', 'routing', 'reflection', 'decision']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['rag', 'long-context', 'llm']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Failure types bonuses.\n",
        "            if 'failure' in query:\n",
        "                if any(term in sentence_lower for term in ['failure', 'error', 'problem']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['multi-step', 'general', 'implicit', 'complex']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Metrics bonuses.\n",
        "            if 'metric' in query:\n",
        "                if any(term in sentence_lower for term in ['mrr', 'recall@', 'ndcg', 'precision']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['evaluation', 'performance', 'score']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Chunking bonuses.\n",
        "            if 'chunking' in query:\n",
        "                if any(term in sentence_lower for term in ['chunk', 'segment', 'overlap']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['trade-off', 'balance', 'cost']):\n",
        "                    bonus += 2\n",
        "\n",
        "            total_score = overlap + bonus\n",
        "\n",
        "            if total_score >= 3:\n",
        "                scored_sentences.append((total_score, sentence.strip()))\n",
        "\n",
        "        if scored_sentences:\n",
        "            scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "            top_sentences = [sent for _, sent in scored_sentences[:3]]\n",
        "            return \". \".join(top_sentences) + \".\"\n",
        "\n",
        "        return \"Based on the available documents no information was found.\"\n",
        "\n",
        "    def contains_answer_to_query(self, text: str, query: str) -> bool:\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Patterns for different query types.\n",
        "        if 'self-route' in query and 'goal' in query:\n",
        "            return any(term in text_lower for term in ['self-route', 'routing', 'goal', 'objective', 'purpose'])\n",
        "\n",
        "        if 'failure' in query and 'rag' in query:\n",
        "            return any(term in text_lower for term in ['failure', 'error', 'problem', 'issue', 'case'])\n",
        "\n",
        "        if 'metric' in query and 'embedding' in query:\n",
        "            return any(term in text_lower for term in ['mrr', 'recall', 'ndcg', 'metric', 'evaluation'])\n",
        "\n",
        "        if 'reranking' in query:\n",
        "            return any(term in text_lower for term in ['rerank', 'ranking', 'reorder'])\n",
        "\n",
        "        if 'chunking' in query and 'trade-off' in query:\n",
        "            return any(term in text_lower for term in ['chunk', 'segment', 'trade-off', 'balance'])\n",
        "\n",
        "        if 'multimodal' in query:\n",
        "            return any(term in text_lower for term in ['multimodal', 'multi-modal', 'cross-modal'])\n",
        "\n",
        "        query_words = set(query.split())\n",
        "        text_words = set(text_lower.split())\n",
        "        overlap = len(query_words.intersection(text_words))\n",
        "\n",
        "        return overlap >= 2\n",
        "\n",
        "# Checking if sentence is relevant to the query.\n",
        "    def is_sentence_relevant(self, sentence: str, query: str) -> bool:\n",
        "        query_words = set(query.split())\n",
        "        sentence_words = set(sentence.split())\n",
        "\n",
        "        overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "        if overlap >= 3:\n",
        "            return True\n",
        "\n",
        "        if overlap >= 2:\n",
        "            # Checking for academic terms.\n",
        "            if any(term in sentence for term in ['method', 'approach', 'technique', 'result', 'finding', 'performance']):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 8) -> Dict:\n",
        "        try:\n",
        "            # Retrieving relevant chunks with expansion.\n",
        "            relevant_chunks = self.embedding_manager.retrieve_relevant_chunks(query, top_k)\n",
        "\n",
        "            context = self.create_focused_context(relevant_chunks, query, max_length=1200)  # Increased context\n",
        "\n",
        "            answer = self.generate_extractive_answer(query, context)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'relevant_chunks': relevant_chunks,\n",
        "                'context_used': context\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in generate_answer: {e}\")\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': f\"Error processing query: {str(e)}\",\n",
        "                'relevant_chunks': [],\n",
        "                'context_used': \"\"\n",
        "            }\n",
        "\n",
        "# TemplateRAGSystem as backup.\n",
        "class TemplateRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "        self.templates = {\n",
        "            'method_goal': \"Based on the research, {content}. The main objective is {goals}.\",\n",
        "            'comparison': \"The research shows {content}. Key differences include {comparison_details}.\",\n",
        "            'evaluation': \"The evaluation results indicate {content}. Metrics show {metrics}.\",\n",
        "            'failure_analysis': \"The identified issues include {content}. These occur when {conditions}.\",\n",
        "            'default': \"According to the research: {content}\"\n",
        "        }\n",
        "\n",
        "    def setup_documents(self, documents: Dict[str, str]):\n",
        "        chunker = TextChunker(chunk_size=400, overlap=80)\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "        self.embedding_manager.create_embeddings(chunks)\n",
        "        print(\"Template RAG system ready.\")\n",
        "\n",
        "    def identify_query_type(self, query: str) -> str:\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(word in query_lower for word in ['goal', 'purpose', 'objective', 'method']):\n",
        "            return 'method_goal'\n",
        "        elif any(word in query_lower for word in ['compare', 'versus', 'difference', 'better']):\n",
        "            return 'comparison'\n",
        "        elif any(word in query_lower for word in ['evaluate', 'metric', 'performance', 'result']):\n",
        "            return 'evaluation'\n",
        "        elif any(word in query_lower for word in ['failure', 'error', 'problem', 'issue']):\n",
        "            return 'failure_analysis'\n",
        "        else:\n",
        "            return 'default'\n",
        "\n",
        "    def extract_key_information(self, chunks: List[Dict]) -> Dict[str, str]:\n",
        "        combined_text = \" \".join([chunk['text'] for chunk in chunks])\n",
        "\n",
        "        return {\n",
        "            'content': self.get_most_relevant_content(combined_text),\n",
        "            'goals': self.extract_goals(combined_text),\n",
        "            'comparison_details': self.extract_comparisons(combined_text),\n",
        "            'metrics': self.extract_metrics(combined_text),\n",
        "            'conditions': self.extract_conditions(combined_text)\n",
        "        }\n",
        "\n",
        "    def get_most_relevant_content(self, text: str) -> str:\n",
        "        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 25]\n",
        "        return '. '.join(sentences[:4]) + '.' if sentences else text[:300]\n",
        "\n",
        "    def extract_goals(self, text: str) -> str:\n",
        "        goal_patterns = ['goal', 'objective', 'aim', 'purpose', 'method', 'approach']\n",
        "        sentences = text.split('.')\n",
        "        goal_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in goal_patterns)]\n",
        "        return '. '.join(goal_sentences[:2]) + '.' if goal_sentences else \"achieving improved performance\"\n",
        "\n",
        "    def extract_comparisons(self, text: str) -> str:\n",
        "        comparison_patterns = ['compare', 'versus', 'better', 'superior', 'outperform', 'difference']\n",
        "        sentences = text.split('.')\n",
        "        comparison_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in comparison_patterns)]\n",
        "        return '. '.join(comparison_sentences[:2]) + '.' if comparison_sentences else \"performance differences observed\"\n",
        "\n",
        "    def extract_metrics(self, text: str) -> str:\n",
        "        metric_patterns = ['metric', 'performance', 'accuracy', 'precision', 'recall', 'mrr', 'ndcg']\n",
        "        sentences = text.split('.')\n",
        "        metric_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in metric_patterns)]\n",
        "        return '. '.join(metric_sentences[:2]) + '.' if metric_sentences else \"evaluation metrics applied\"\n",
        "\n",
        "    def extract_conditions(self, text: str) -> str:\n",
        "        condition_patterns = ['when', 'condition', 'case', 'scenario', 'context']\n",
        "        sentences = text.split('.')\n",
        "        condition_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in condition_patterns)]\n",
        "        return '. '.join(condition_sentences[:2]) + '.' if condition_sentences else \"specific conditions apply\"\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 6) -> Dict:\n",
        "        try:\n",
        "            relevant_chunks = self.embedding_manager.retrieve_relevant_chunks(query, top_k)\n",
        "            query_type = self.identify_query_type(query)\n",
        "            template = self.templates[query_type]\n",
        "\n",
        "            info = self.extract_key_information(relevant_chunks)\n",
        "            answer = template.format(**info)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'relevant_chunks': relevant_chunks,\n",
        "                'query_type': query_type\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': f\"Based on the available information: {str(e)}\",\n",
        "                'relevant_chunks': [],\n",
        "                'query_type': 'error'\n",
        "            }"
      ],
      "metadata": {
        "id": "GNgYEIOZZcKJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Processing Documents\")\n",
        "    doc_processor = DocumentProcessor()\n",
        "    documents = doc_processor.upload_and_process_pdfs()\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No documents uploaded.\")\n",
        "        return\n",
        "\n",
        "    print(\"Setting Up RAG Systems\")\n",
        "\n",
        "    # Use improved RAG system as primary\n",
        "    improved_rag = ImprovedRAGSystem()\n",
        "    improved_rag.setup_documents(documents)\n",
        "\n",
        "    # Keep template system as backup\n",
        "    template_rag = TemplateRAGSystem()\n",
        "    template_rag.setup_documents(documents)\n",
        "\n",
        "    print(\"Testing with Improved System\")\n",
        "\n",
        "    test_queries = [\n",
        "        \"What is the primary goal of the SELF-ROUTE method proposed by Zhuowan Li?\",\n",
        "        \"Explain why the researchers believe RAG might still be useful despite the superior performance of long-context LLMs\",\n",
        "        \"Compare the reranking techniques mentioned in the Wang paper. How do they impact the retrieval quality?\",\n",
        "        \"What are the trade-offs involved when using different chunking strategies in RAG systems?\",\n",
        "        \"How does multimodal retrieval enhance the capabilities of RAG?\",\n",
        "        \"What were the key failure cases for RAG in handling long context retrievals, as noted by Zhuowan Li?\",\n",
        "        \"Why does the Zhuowan paper claim that long-context LLMs outperformed RAG in most cases? What benefits does RAG still offer?\",\n",
        "        \"Describe the metrics used to evaluate the different embedding models for RAG in Wang's paper\",\n",
        "        \"Discuss the implications of using self-reflection in routing queries between RAG and long-context LLMs\",\n",
        "        \"How does query rewriting contribute to the overall efficiency of RAG according to Wang's findings?\"\n",
        "    ]\n",
        "\n",
        "    improved_results = []\n",
        "    for query in test_queries:\n",
        "        result = improved_rag.generate_answer(query)\n",
        "        improved_results.append(result)\n",
        "        print(f\"\\nQ: {query}\")\n",
        "        print(f\"A: {result['answer']}\")\n",
        "        print(f\"Retrieved chunks: {len(result['relevant_chunks'])}\")\n",
        "        if result['relevant_chunks']:\n",
        "            print(f\"Top similarity: {result['relevant_chunks'][0]['similarity_score']:.3f}\")\n",
        "\n",
        "    # Save results\n",
        "    results_df = pd.DataFrame([\n",
        "        {\n",
        "            'Query': r['query'],\n",
        "            'Answer': r['answer'],\n",
        "            'Num_Chunks_Retrieved': len(r['relevant_chunks']),\n",
        "            'Top_Similarity_Score': r['relevant_chunks'][0]['similarity_score'] if r['relevant_chunks'] else 0,\n",
        "        }\n",
        "        for r in improved_results\n",
        "    ])\n",
        "\n",
        "    results_df.to_csv('improved_rag_results.csv', index=False)\n",
        "    print(f\"\\nResults saved to 'improved_rag_results.csv'\")\n",
        "\n",
        "    return improved_rag, template_rag, improved_results\n",
        "\n",
        "def test_query(rag_system, query: str):\n",
        "    print(f\"Testing Query\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "    result = rag_system.generate_answer(query)\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Chunks retrieved: {len(result['relevant_chunks'])}\")\n",
        "    if result['relevant_chunks']:\n",
        "        print(f\"Top similarity: {result['relevant_chunks'][0]['similarity_score']:.3f}\")\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    improved_rag, template_rag, improved_results = main()\n",
        "\n",
        "    # Test with specific problematic query\n",
        "    test_result = test_query(improved_rag, \"What are the four failure types for RAG identified by Zhuowan Li?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "feacf9d02531409ba7eca34098405682",
            "e54271a4198f4d2992d56f83c58f6dea",
            "a3178852ed124aafb01a6becfc21bff8",
            "3dd860a332ab44119e8b78930e4a6a29",
            "b8c609ccfd4a491baa467aea7d0d6d6c",
            "f540f67562a846bea04dc1ff4c7a8846",
            "8eb8c200cd884f1d8c0c895e54fdb5bf",
            "124f8a30c10c4623a40de1418cf13e77",
            "6b7aed4972854f5fbecf2413bf300d5e",
            "9d91ba16a96a41edadfc43610738399b",
            "8327f6442ddd4bd28ab08998f8947d93",
            "c1c7ca5ab0994d339e36b820e4958de7",
            "e7dca4810090424586f78562d92b830e",
            "7ac809ad0c0a4850a5002a3a89c65f98",
            "67248c8bbf834240b6820e86fb78fa3c",
            "3d4d0cdf9f9045eeb9dd072f76ba1c46",
            "7424d5d187324be0bc35367bb0841516",
            "b939b9b9266942a39e016244ba0cef05",
            "b36c193ffe2e4193b88a7397a8020c2d",
            "90e99468dd6142b8b010ce9ff500149a",
            "0df985cdf7de457198f4e8b57e50382f",
            "63f0254966ad41ee893c73cd3de05016"
          ]
        },
        "id": "QMdh6QQIWQst",
        "outputId": "d9ea8b3b-e43f-4e3f-a639-e3a3276ddce1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Documents\n",
            "Upload the PDF file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75a15ffd-e60e-4a53-bfb4-02d177d452f1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75a15ffd-e60e-4a53-bfb4-02d177d452f1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2407.pdf to 2407 (9).pdf\n",
            "Processed 2407 (9).pdf: 127056 characters\n",
            "Setting Up RAG Systems\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator loaded successfully.\n",
            "Created 61 chunks total\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feacf9d02531409ba7eca34098405682"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 61 embeddings with dimension 384\n",
            "RAG system ready.\n",
            "Created 61 chunks total\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1c7ca5ab0994d339e36b820e4958de7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 61 embeddings with dimension 384\n",
            "Template RAG system ready.\n",
            "Testing with Improved System\n",
            "\n",
            "Q: What is the primary goal of the SELF-ROUTE method proposed by Zhuowan Li?\n",
            "A: [Source 1]: This is because when kin- SELF_ROUTE_CONTENT: creases, the cost of RAG (and routing) increases, but more queries are routed to RAG from LC, thus the overall cost may decrease... PERFORMANCE_COMPARISON: To gain a better understanding of why RAG lags\n",
            "behind LC, we analyze the failure reasons for the\n",
            "examples that cannot be answered by RAG... [Source 2]: Noticeably, the performance\n",
            "gap is more significant for the more recent mod-\n",
            "els (GPT-4O and Gemini-1.5-Pro) compared to\n",
            "GPT-3.5-Turbo, highlighting the exceptional long-\n",
            "context understanding capacity of the latest LLMs...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.481\n",
            "\n",
            "Q: Explain why the researchers believe RAG might still be useful despite the superior performance of long-context LLMs\n",
            "A: [Source 2]: A.6 Experimental Details of Comprehensive Evaluation\n",
            "Tasks and Datasets We conducted extensive experiments across various NLP tasks and datasets to\n",
            "assess the performance of RAG systems... [Source 1]: Ruler: Whats the real context size of\n",
            "your long-context language models?.. We use a more lenient EM score, which evaluates performance based on\n",
            "whether the model generations include gold answers instead of strictly exact matching [74]...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.542\n",
            "\n",
            "Q: Compare the reranking techniques mentioned in the Wang paper. How do they impact the retrieval quality?\n",
            "A: [Source 2]: We compare our results with a randomly\n",
            "METHOD_GOAL: shuffled ordering and the BM25 retrieval baseline...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.675\n",
            "\n",
            "Q: What are the trade-offs involved when using different chunking strategies in RAG systems?\n",
            "A: 2.2 Retriever Enhancement Strategy CHUNKING_STRATEGY: Document chunking and embedding methods significantly impact retrieval performance...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.506\n",
            "\n",
            "Q: How does multimodal retrieval enhance the capabilities of RAG?\n",
            "A: [Source 2]: We demonstrate that the integration of multimodal retrieval techniques can substantially improve\n",
            "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
            "through a strategy of retrieval as generation... [Source 1]: These multimodal RAG capabilities offer the following advantages: METHOD_GOAL: Groundedness : Retrieval methods provide information from verified multimodal materials, thereby ensuring authenticity and specificity... query aligns well with the textual descriptions of stored images (i.e., retrieval as generation strategy), while the image2text functionality comes into play when a user provides an image and engages in conversation about the input image...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.553\n",
            "\n",
            "Q: What were the key failure cases for RAG in handling long context retrievals, as noted by Zhuowan Li?\n",
            "A: [Source 1]: PERFORMANCE_COMPARISON: 3 Benchmarking RAG versus LC\n",
            "3.1 Datasets and metrics\n",
            "We evaluate on a subset of datasets from Long-\n",
            "Bench (Bai et al., 2023) and Bench (Zhang et al.,\n",
            "2024), which are recent benchmarks containing a\n",
            "collection of new and existing datasets for LLM\n",
            "evaluation, covering both synthetic and real texts in\n",
            "multiple languages... Evaluating long-context models is challenging due to the difficulty in collecting and analyzing long texts...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.594\n",
            "\n",
            "Q: Why does the Zhuowan paper claim that long-context LLMs outperformed RAG in most cases? What benefits does RAG still offer?\n",
            "A: Based on the available documents no information was found.\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.542\n",
            "\n",
            "Q: Describe the metrics used to evaluate the different embedding models for RAG in Wang's paper\n",
            "A: [Source 2]: METHOD_GOAL: In this study, we aim to identify the best practices for RAG through extensive experimentation... To the best of our knowledge, there has been no systematic effort to pursue the optimal implementation of RAG, particularly for the entire RAG workflow... First, we compare representative methods for each RAG step (or\n",
            "METHOD_GOAL: module) and select up to three of the best-performing methods...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.476\n",
            "\n",
            "Q: Discuss the implications of using self-reflection in routing queries between RAG and long-context LLMs\n",
            "A: [Source 1]: In addition to\n",
            "quantitative evaluation, we provide a comprehen-\n",
            "sive analysis comparing RAG and LC, including\n",
            "common failure patterns of RAG, the trade-offs\n",
            "between cost and performance, and the results on\n",
            "additional synthetic datasets... 1 shows the comparisons of LC, RAG and SELF_ROUTE_CONTENT: SELF-ROUTE using three recent LLMs: GPT-4O, GPT-3.5-Turbo and Gemini-1.5-Pro... Our analysis serves\n",
            "as a starting point, inspiring future improvements\n",
            "of RAG, and as a empirical guide for building long-\n",
            "context applications using RAG and LC...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.605\n",
            "\n",
            "Q: How does query rewriting contribute to the overall efficiency of RAG according to Wang's findings?\n",
            "A: [Source 1]: The experimental results demonstrate that each module contributes uniquely to the overall perfor-\n",
            "mance of the RAG system... 4.2 Results and Analysis\n",
            "Based on the experimental results presented in Table 11, the following key insights emerge:\n",
            "Query Classification Module: This module is referenced and contributes to both effectiveness\n",
            "and efficiency, leading to an average improvement in the overall score from 0.428to0.443and a\n",
            "reduction in latency time from 16.41to11.58seconds per query... 11.Page 12 METHOD_GOAL: Retrieval Module: While the Hybrid with HyDE method attained the highest RAG score of\n",
            "0.58, it does so at a considerable computational cost with 11.71second per query...\n",
            "Retrieved chunks: 8\n",
            "Top similarity: 0.558\n",
            "\n",
            "Results saved to 'improved_rag_results.csv'\n",
            "Testing Query\n",
            "Query: 'What are the four failure types for RAG identified by Zhuowan Li?'\n",
            "Answer: [Source 1]: 5 Discussion\n",
            "5.1 Best Practices for Implementing RAG\n",
            "According to our experimental findings, we suggest two distinct recipes or practices for implementing\n",
            "RAG systems, each customized to address specific requirements: one focusing on maximizing\n",
            "performance, and the other on striking a balance between efficiency and efficacy...\n",
            "Chunks retrieved: 8\n",
            "Top similarity: 0.433\n"
          ]
        }
      ]
    }
  ]
}