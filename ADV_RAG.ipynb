{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First, install additional dependencies\n",
        "!pip install sentence-transformers[train] datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHEKohaywjmt",
        "outputId": "b210bc75-db51-4bf3-c42c-44a512184186"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: sentence-transformers[train] in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers[train]) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers[train]) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers[train]) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.9)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers[train])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers[train]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers[train]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers[train]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers[train]) (0.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers[train]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers[train]) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers[train]) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers faiss-cpu PyPDF2 torch openai python-dotenv\n",
        "!pip install --upgrade langchain langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8BYHKPKhxU9",
        "outputId": "cb35077a-c5b4-48af-856e-b3eb22b70018"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-dotenv-1.1.1\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "import faiss\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tVg4PiZ4iX2B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
        "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "c8t8dOWTwm5R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# My function is handling the PDF and the text extraction.\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.documents = {}\n",
        "\n",
        "    # Extracting the text from the file.\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    page_text = page.extract_text()\n",
        "                    # Table detection and formatting.\n",
        "                    page_text = self.enhance_table_extraction(page_text)\n",
        "                    text += f\"\\Page {page_num + 1} {page_text}.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "        return text\n",
        "\n",
        "# Table formatting.\n",
        "    def enhance_table_extraction(self, text: str) -> str:\n",
        "        \"\"\"Aggressively improve table formatting and preserve critical academic content\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        processed_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Self-route method.\n",
        "            if re.search(r'(self-route|routing|self-reflection)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"SELF_ROUTE_CONTENT: {line}\")\n",
        "\n",
        "            # Failure type lists.\n",
        "            elif re.search(r'(failure|error).*(type|case|category)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"FAILURE_TYPES: {line}\")\n",
        "            elif re.search(r'(multi-step|general knowledge|implicit|long.?complex)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"FAILURE_DETAIL: {line}\")\n",
        "\n",
        "            # Evaluation metrics and tables.\n",
        "            elif re.search(r'(mrr|recall@|ndcg@|precision|f1)', line, re.IGNORECASE):\n",
        "                # Keeping the table structure with space.\n",
        "                line = re.sub(r'\\s+', ' | ', line)\n",
        "                processed_lines.append(f\"METRICS_TABLE: {line}\")\n",
        "\n",
        "            # Chunking strategy content.\n",
        "            elif re.search(r'(chunk|segment|overlap|window)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"CHUNKING_STRATEGY: {line}\")\n",
        "\n",
        "            # Performance comparisons.\n",
        "            elif re.search(r'(outperform|superior|better|vs|versus|comparison)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"PERFORMANCE_COMPARISON: {line}\")\n",
        "\n",
        "            # Method objectives and goals.\n",
        "            elif re.search(r'(goal|objective|aim|purpose|method)', line, re.IGNORECASE):\n",
        "                processed_lines.append(f\"METHOD_GOAL: {line}\")\n",
        "\n",
        "            else:\n",
        "                processed_lines.append(line)\n",
        "\n",
        "        return '\\n'.join(processed_lines)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "        # Preserve important academic patterns\n",
        "        text = re.sub(r'([.!?])\\s+([A-Z])', r'\\1\\n\\2', text)  # Sentence boundaries\n",
        "\n",
        "        # Keep important punctuation and academic notation\n",
        "        text = re.sub(r'[^\\w\\s.,;:!?()%@\\-\\[\\]{}|]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "# Cleaning the text by removing extra whitespaces, special characters while keeping punctations.\n",
        "    def upload_and_process_pdfs(self) -> Dict[str, str]:\n",
        "        print(\"Upload the PDF file.\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        for filename, content in uploaded.items():\n",
        "            if filename.endswith('.pdf'):\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(content)\n",
        "\n",
        "                text = self.extract_text_from_pdf(filename)\n",
        "                cleaned_text = self.clean_text(text)\n",
        "                self.documents[filename] = cleaned_text\n",
        "                print(f\"Processed {filename}: {len(cleaned_text)} characters\")\n",
        "\n",
        "        return self.documents"
      ],
      "metadata": {
        "id": "GSlzNNhMi29o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the text chunking with overlap.\n",
        "class TextChunker:\n",
        "    def __init__(self, chunk_size: int = 512, overlap: int = 100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "# Splitting by paragraphs first, then by sentences for better coherence.\n",
        "    def chunk_text(self, text: str, document_name: str) -> List[Dict]:\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        chunks = []\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        word_count = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_words = sentence.split()\n",
        "\n",
        "                if word_count + len(sentence_words) > self.chunk_size and current_chunk:\n",
        "                    chunks.append({\n",
        "                        'text': current_chunk.strip(),\n",
        "                        'document': document_name,\n",
        "                        'chunk_id': len(chunks),\n",
        "                        'word_count': word_count\n",
        "                    })\n",
        "\n",
        "       # New chunk with overlap.\n",
        "                    overlap_text = ' '.join(current_chunk.split()[-self.overlap:])\n",
        "                    current_chunk = overlap_text + \" \" + sentence\n",
        "                    word_count = len(current_chunk.split())\n",
        "                else:\n",
        "                    current_chunk += \" \" + sentence\n",
        "                    word_count += len(sentence_words)\n",
        "\n",
        "        # Final chunk.\n",
        "        if current_chunk.strip():\n",
        "            chunks.append({\n",
        "                'text': current_chunk.strip(),\n",
        "                'document': document_name,\n",
        "                'chunk_id': len(chunks),\n",
        "                'word_count': word_count\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def chunk_documents(self, documents: Dict[str, str]) -> List[Dict]:\n",
        "        all_chunks = []\n",
        "        for doc_name, text in documents.items():\n",
        "            chunks = self.chunk_text(text, doc_name)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"Created {len(all_chunks)} chunks total\")\n",
        "        return all_chunks"
      ],
      "metadata": {
        "id": "EUWBSKWTi3jN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document embeddings and retrieving.\n",
        "class EmbeddingManager:\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.embeddings = None\n",
        "        self.chunks = None\n",
        "        self.index = None\n",
        "\n",
        "    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:\n",
        "        texts = [chunk['text'] for chunk in chunks]\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        self.chunks = chunks\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        self.index.add(embeddings_normalized.astype('float32'))\n",
        "\n",
        "        print(f\"Created {len(embeddings)} embeddings with dimension {dimension}\")\n",
        "        return embeddings\n",
        "\n",
        "# For targeted query expansion to find more specific content.\n",
        "    def expand_query(self, query: str) -> List[str]:\n",
        "        base_query = query.lower()\n",
        "        expanded_queries = [query]\n",
        "\n",
        "        # Specific targeting.\n",
        "        if 'self-route' in base_query and 'goal' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"SELF-ROUTE self-reflection routing decision\",\n",
        "                \"model self-reflection dynamically route queries\",\n",
        "                \"routing between RAG and LC cost context length\",\n",
        "                \"Zhuowan Li SELF-ROUTE method objective\",\n",
        "                \"self-reflection mechanism route queries RAG long-context\"\n",
        "            ])\n",
        "\n",
        "        # Failure types.\n",
        "        if 'failure' in base_query and ('four' in base_query or 'types' in base_query or 'cases' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"four failure types RAG multi-step general implicit long complex\",\n",
        "                \"Multi-step reasoning failure General knowledge failure\",\n",
        "                \"Implicit knowledge failure Long complex context failure\",\n",
        "                \"failure categories RAG handling long context\",\n",
        "                \"Zhuowan Li four key failure cases\"\n",
        "            ])\n",
        "\n",
        "        # Targeting Wang's paper.\n",
        "        if 'chunking' in base_query and 'trade-off' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"chunking strategies overlap non-overlap performance cost\",\n",
        "                \"Wang paper chunking module section\",\n",
        "                \"chunk size overlap impact retrieval performance\",\n",
        "                \"segmentation strategies recall faithfulness trade-offs\",\n",
        "                \"chunking overlap computational efficiency\"\n",
        "            ])\n",
        "\n",
        "        # Targeting Wang's evaluation section.\n",
        "        if 'embedding' in base_query and ('metric' in base_query or 'evaluate' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"Wang embedding models MRR Recall@5 Recall@10 nDCG@10\",\n",
        "                \"BGE LLM-Embedder evaluation metrics comparison\",\n",
        "                \"embedding model performance evaluation Wang\",\n",
        "                \"retrieval evaluation metrics MRR nDCG recall\",\n",
        "                \"Wang paper embedding evaluation results\"\n",
        "            ])\n",
        "\n",
        "        # Reranking.\n",
        "        if 'reranking' in base_query or 'rerank' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"Wang reranking techniques retrieval quality impact\",\n",
        "                \"reranking methods comparison Wang paper\",\n",
        "                \"retrieval reranking performance improvement\"\n",
        "            ])\n",
        "\n",
        "\n",
        "        if ('rag' in base_query and 'useful' in base_query) or ('despite' in base_query and 'superior' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"RAG cheaper cost efficient overlap 63% LC models\",\n",
        "                \"RAG benefits despite long-context LLM superiority\",\n",
        "                \"cost efficiency RAG vs long-context LLMs\",\n",
        "                \"why RAG still useful cheaper computational cost\"\n",
        "            ])\n",
        "\n",
        "        # LC vs RAG performance.\n",
        "        if 'long-context' in base_query and ('outperform' in base_query or 'superior' in base_query):\n",
        "            expanded_queries.extend([\n",
        "                \"long-context LLMs outperformed RAG complex reasoning\",\n",
        "                \"Zhuowan LC LLM superior performance RAG efficiency\",\n",
        "                \"long-context models better complex multi-document queries\"\n",
        "            ])\n",
        "\n",
        "        # Multimodal capabilities.\n",
        "        if 'multimodal' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"multimodal retrieval cross-modal capabilities enhancement\",\n",
        "                \"text image multimodal search RAG\",\n",
        "                \"multimodal RAG vision language models\"\n",
        "            ])\n",
        "\n",
        "        # Rewriting queries.\n",
        "        if 'query rewriting' in base_query or 'query enhancement' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"Wang query rewriting efficiency findings\",\n",
        "                \"query enhancement reformulation techniques Wang\",\n",
        "                \"query rewriting impact RAG efficiency\"\n",
        "            ])\n",
        "\n",
        "        # Self-reflection.\n",
        "        if 'implications' in base_query and 'self-reflection' in base_query:\n",
        "            expanded_queries.extend([\n",
        "                \"self-reflection routing implications consequences\",\n",
        "                \"adaptive routing decision impact analysis\",\n",
        "                \"SELF-ROUTE self-reflection routing benefits\"\n",
        "            ])\n",
        "\n",
        "        return expanded_queries\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query: str, top_k: int = 8) -> List[Dict]:  # Increased top_k\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not created\")\n",
        "\n",
        "        # Query expansion.\n",
        "        expanded_queries = self.expand_query(query)\n",
        "        all_results = []\n",
        "\n",
        "        for exp_query in expanded_queries:\n",
        "            query_embedding = self.model.encode([exp_query])\n",
        "            query_normalized = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "\n",
        "            scores, indices = self.index.search(query_normalized.astype('float32'), top_k)\n",
        "\n",
        "            for score, idx in zip(scores[0], indices[0]):\n",
        "                chunk = self.chunks[idx].copy()\n",
        "                chunk['similarity_score'] = float(score)\n",
        "                chunk['query_variant'] = exp_query\n",
        "                all_results.append(chunk)\n",
        "\n",
        "        # Removing duplicates and sorting by score.\n",
        "        seen_chunks = set()\n",
        "        unique_results = []\n",
        "        for result in all_results:\n",
        "            chunk_id = result['chunk_id']\n",
        "            if chunk_id not in seen_chunks:\n",
        "                seen_chunks.add(chunk_id)\n",
        "                unique_results.append(result)\n",
        "\n",
        "        # Sortinf by the similarity score.\n",
        "        unique_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
        "        return unique_results[:top_k]"
      ],
      "metadata": {
        "id": "rA-t0ER9jJcv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG system.\n",
        "class ImprovedRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "        self.generator = None\n",
        "        self.setup_generator()\n",
        "\n",
        "    def setup_generator(self):\n",
        "        try:\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=\"distilgpt2\",\n",
        "                tokenizer=\"distilgpt2\",\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                return_full_text=False,\n",
        "                max_new_tokens=150,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=50256\n",
        "            )\n",
        "            print(\"Generator loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading generator: {e}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def setup_documents(self, documents: Dict[str, str]):\n",
        "        chunker = TextChunker(chunk_size=400, overlap=80)\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "        self.embedding_manager.create_embeddings(chunks)\n",
        "        print(\"RAG system ready.\")\n",
        "\n",
        "# Focused context based on the query type.\n",
        "    def create_focused_context(self, chunks: List[Dict], query: str, max_length: int = 1000) -> str:\n",
        "        context_parts = []\n",
        "        current_length = 0\n",
        "\n",
        "        # Prioritizing chunks with higher similarity scores.\n",
        "        sorted_chunks = sorted(chunks, key=lambda x: x['similarity_score'], reverse=True)\n",
        "\n",
        "        for i, chunk in enumerate(sorted_chunks):\n",
        "            # Extracting the most relevant sentences from each chunk.\n",
        "            relevant_sentences = self.extract_relevant_sentences(chunk['text'], query)\n",
        "\n",
        "            if relevant_sentences:\n",
        "                chunk_context = f\"[Source {i+1}]: {relevant_sentences}\"\n",
        "\n",
        "                if current_length + len(chunk_context) > max_length:\n",
        "                    break\n",
        "\n",
        "                context_parts.append(chunk_context)\n",
        "                current_length += len(chunk_context)\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def extract_relevant_sentences(self, text: str, query: str) -> str:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        scored_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 15:\n",
        "                continue\n",
        "\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            # Calculating the overlap score.\n",
        "            overlap = len(query_words.intersection(sentence_words))\n",
        "            if any(term in sentence.lower() for term in ['rag', 'llm', 'retrieval', 'embedding', 'failure', 'performance']):\n",
        "                overlap += 1\n",
        "\n",
        "            if overlap > 0:\n",
        "                scored_sentences.append((overlap, sentence))\n",
        "\n",
        "        # Sorting by relevance.\n",
        "        scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "        top_sentences = [sent for _, sent in scored_sentences[:3]]\n",
        "\n",
        "        return \". \".join(top_sentences) + \".\" if top_sentences else \"\"\n",
        "\n",
        "    def generate_answer_with_reasoning(self, query: str, context: str) -> str:\n",
        "        \"\"\"Skip generative approach, it's causing hallucinations\"\"\"\n",
        "        return \"\"\n",
        "\n",
        "    def generate_extractive_answer(self, query: str, context: str) -> str:\n",
        "        if not context:\n",
        "            return \"No relevant information found in the documents.\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Specially tagged content first.\n",
        "        tagged_content = self.extract_tagged_content(context, query_lower)\n",
        "        if tagged_content:\n",
        "            return tagged_content\n",
        "\n",
        "        # Fallback to normal extraction.\n",
        "        return self.extract_regular_content(context, query_lower)\n",
        "\n",
        " # Extracting content based on special tags.\n",
        "    def extract_tagged_content(self, context: str, query: str) -> str:\n",
        "        lines = context.split('\\n')\n",
        "        relevant_lines = []\n",
        "\n",
        "        # Self-route targeting.\n",
        "        if 'self-route' in query and 'goal' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('SELF_ROUTE_CONTENT:') or line.startswith('METHOD_GOAL:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    if any(term in content.lower() for term in ['route', 'routing', 'reflection', 'decision']):\n",
        "                        relevant_lines.append(content)\n",
        "\n",
        "        # Failure types targeting.\n",
        "        elif 'failure' in query and ('four' in query or 'types' in query or 'cases' in query):\n",
        "            failure_content = []\n",
        "            for line in lines:\n",
        "                if line.startswith('FAILURE_TYPES:') or line.startswith('FAILURE_DETAIL:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    failure_content.append(content)\n",
        "\n",
        "            if failure_content:\n",
        "                enumerated = []\n",
        "                for content in failure_content:\n",
        "                    if re.search(r'(1\\.|first|multi-step)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"1) Multi-step reasoning failure: {content}\")\n",
        "                    elif re.search(r'(2\\.|second|general)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"2) General knowledge failure: {content}\")\n",
        "                    elif re.search(r'(3\\.|third|implicit)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"3) Implicit knowledge failure: {content}\")\n",
        "                    elif re.search(r'(4\\.|fourth|long|complex)', content, re.IGNORECASE):\n",
        "                        enumerated.append(f\"4) Long/complex context failure: {content}\")\n",
        "\n",
        "                if enumerated:\n",
        "                    return \". \".join(enumerated) + \".\"\n",
        "                else:\n",
        "                    return \". \".join(failure_content) + \".\"\n",
        "\n",
        "        # Chunking trade-offs targeting.\n",
        "        elif 'chunking' in query and 'trade-off' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('CHUNKING_STRATEGY:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    if any(term in content.lower() for term in ['overlap', 'trade-off', 'balance', 'cost', 'performance']):\n",
        "                        relevant_lines.append(content)\n",
        "\n",
        "        # Metrics targeting.\n",
        "        elif 'metric' in query and 'embedding' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('METRICS_TABLE:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    relevant_lines.append(content)\n",
        "\n",
        "        # Performance comparison targeting.\n",
        "        elif 'outperform' in query or 'superior' in query:\n",
        "            for line in lines:\n",
        "                if line.startswith('PERFORMANCE_COMPARISON:'):\n",
        "                    content = line.split(':', 1)[1].strip()\n",
        "                    relevant_lines.append(content)\n",
        "\n",
        "        if relevant_lines:\n",
        "            unique_lines = []\n",
        "            seen = set()\n",
        "            for line in relevant_lines:\n",
        "                if line not in seen:\n",
        "                    unique_lines.append(line)\n",
        "                    seen.add(line)\n",
        "\n",
        "            return \". \".join(unique_lines[:3]) + \".\"\n",
        "\n",
        "        return \"\"\n",
        "\n",
        " # Extracting the content using normal sentence matching.\n",
        "    def extract_regular_content(self, context: str, query: str) -> str:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "        scored_sentences = []\n",
        "\n",
        "        query_words = set(query.split())\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 25:\n",
        "                continue\n",
        "\n",
        "            sentence_lower = sentence.lower()\n",
        "            sentence_words = set(sentence_lower.split())\n",
        "\n",
        "            # Relevance score.\n",
        "            overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "            bonus = 0\n",
        "\n",
        "            # Self-route bonuses.\n",
        "            if 'self-route' in query:\n",
        "                if any(term in sentence_lower for term in ['self-route', 'routing', 'reflection', 'decision']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['rag', 'long-context', 'llm']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Failure types bonuses.\n",
        "            if 'failure' in query:\n",
        "                if any(term in sentence_lower for term in ['failure', 'error', 'problem']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['multi-step', 'general', 'implicit', 'complex']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Metrics bonuses.\n",
        "            if 'metric' in query:\n",
        "                if any(term in sentence_lower for term in ['mrr', 'recall@', 'ndcg', 'precision']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['evaluation', 'performance', 'score']):\n",
        "                    bonus += 2\n",
        "\n",
        "            # Chunking bonuses.\n",
        "            if 'chunking' in query:\n",
        "                if any(term in sentence_lower for term in ['chunk', 'segment', 'overlap']):\n",
        "                    bonus += 3\n",
        "                if any(term in sentence_lower for term in ['trade-off', 'balance', 'cost']):\n",
        "                    bonus += 2\n",
        "\n",
        "            total_score = overlap + bonus\n",
        "\n",
        "            if total_score >= 3:\n",
        "                scored_sentences.append((total_score, sentence.strip()))\n",
        "\n",
        "        if scored_sentences:\n",
        "            scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "            top_sentences = [sent for _, sent in scored_sentences[:3]]\n",
        "            return \". \".join(top_sentences) + \".\"\n",
        "\n",
        "        return \"Based on the available documents no information was found.\"\n",
        "\n",
        "    def contains_answer_to_query(self, text: str, query: str) -> bool:\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Patterns for different query types.\n",
        "        if 'self-route' in query and 'goal' in query:\n",
        "            return any(term in text_lower for term in ['self-route', 'routing', 'goal', 'objective', 'purpose'])\n",
        "\n",
        "        if 'failure' in query and 'rag' in query:\n",
        "            return any(term in text_lower for term in ['failure', 'error', 'problem', 'issue', 'case'])\n",
        "\n",
        "        if 'metric' in query and 'embedding' in query:\n",
        "            return any(term in text_lower for term in ['mrr', 'recall', 'ndcg', 'metric', 'evaluation'])\n",
        "\n",
        "        if 'reranking' in query:\n",
        "            return any(term in text_lower for term in ['rerank', 'ranking', 'reorder'])\n",
        "\n",
        "        if 'chunking' in query and 'trade-off' in query:\n",
        "            return any(term in text_lower for term in ['chunk', 'segment', 'trade-off', 'balance'])\n",
        "\n",
        "        if 'multimodal' in query:\n",
        "            return any(term in text_lower for term in ['multimodal', 'multi-modal', 'cross-modal'])\n",
        "\n",
        "        query_words = set(query.split())\n",
        "        text_words = set(text_lower.split())\n",
        "        overlap = len(query_words.intersection(text_words))\n",
        "\n",
        "        return overlap >= 2\n",
        "\n",
        "# Checking if sentence is relevant to the query.\n",
        "    def is_sentence_relevant(self, sentence: str, query: str) -> bool:\n",
        "        query_words = set(query.split())\n",
        "        sentence_words = set(sentence.split())\n",
        "\n",
        "        overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "        if overlap >= 3:\n",
        "            return True\n",
        "\n",
        "        if overlap >= 2:\n",
        "            # Checking for academic terms.\n",
        "            if any(term in sentence for term in ['method', 'approach', 'technique', 'result', 'finding', 'performance']):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 8) -> Dict:\n",
        "        try:\n",
        "            # Retrieving relevant chunks with expansion.\n",
        "            relevant_chunks = self.embedding_manager.retrieve_relevant_chunks(query, top_k)\n",
        "\n",
        "            context = self.create_focused_context(relevant_chunks, query, max_length=1200)  # Increased context\n",
        "\n",
        "            answer = self.generate_extractive_answer(query, context)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'relevant_chunks': relevant_chunks,\n",
        "                'context_used': context\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in generate_answer: {e}\")\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': f\"Error processing query: {str(e)}\",\n",
        "                'relevant_chunks': [],\n",
        "                'context_used': \"\"\n",
        "            }\n",
        "\n",
        "# TemplateRAGSystem as backup.\n",
        "class TemplateRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.embedding_manager = EmbeddingManager()\n",
        "        self.templates = {\n",
        "            'method_goal': \"Based on the research, {content}. The main objective is {goals}.\",\n",
        "            'comparison': \"The research shows {content}. Key differences include {comparison_details}.\",\n",
        "            'evaluation': \"The evaluation results indicate {content}. Metrics show {metrics}.\",\n",
        "            'failure_analysis': \"The identified issues include {content}. These occur when {conditions}.\",\n",
        "            'default': \"According to the research: {content}\"\n",
        "        }\n",
        "\n",
        "    def setup_documents(self, documents: Dict[str, str]):\n",
        "        chunker = TextChunker(chunk_size=400, overlap=80)\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "        self.embedding_manager.create_embeddings(chunks)\n",
        "        print(\"Template RAG system ready.\")\n",
        "\n",
        "    def identify_query_type(self, query: str) -> str:\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        if any(word in query_lower for word in ['goal', 'purpose', 'objective', 'method']):\n",
        "            return 'method_goal'\n",
        "        elif any(word in query_lower for word in ['compare', 'versus', 'difference', 'better']):\n",
        "            return 'comparison'\n",
        "        elif any(word in query_lower for word in ['evaluate', 'metric', 'performance', 'result']):\n",
        "            return 'evaluation'\n",
        "        elif any(word in query_lower for word in ['failure', 'error', 'problem', 'issue']):\n",
        "            return 'failure_analysis'\n",
        "        else:\n",
        "            return 'default'\n",
        "\n",
        "    def extract_key_information(self, chunks: List[Dict]) -> Dict[str, str]:\n",
        "        combined_text = \" \".join([chunk['text'] for chunk in chunks])\n",
        "\n",
        "        return {\n",
        "            'content': self.get_most_relevant_content(combined_text),\n",
        "            'goals': self.extract_goals(combined_text),\n",
        "            'comparison_details': self.extract_comparisons(combined_text),\n",
        "            'metrics': self.extract_metrics(combined_text),\n",
        "            'conditions': self.extract_conditions(combined_text)\n",
        "        }\n",
        "\n",
        "    def get_most_relevant_content(self, text: str) -> str:\n",
        "        sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 25]\n",
        "        return '. '.join(sentences[:4]) + '.' if sentences else text[:300]\n",
        "\n",
        "    def extract_goals(self, text: str) -> str:\n",
        "        goal_patterns = ['goal', 'objective', 'aim', 'purpose', 'method', 'approach']\n",
        "        sentences = text.split('.')\n",
        "        goal_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in goal_patterns)]\n",
        "        return '. '.join(goal_sentences[:2]) + '.' if goal_sentences else \"achieving improved performance\"\n",
        "\n",
        "    def extract_comparisons(self, text: str) -> str:\n",
        "        comparison_patterns = ['compare', 'versus', 'better', 'superior', 'outperform', 'difference']\n",
        "        sentences = text.split('.')\n",
        "        comparison_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in comparison_patterns)]\n",
        "        return '. '.join(comparison_sentences[:2]) + '.' if comparison_sentences else \"performance differences observed\"\n",
        "\n",
        "    def extract_metrics(self, text: str) -> str:\n",
        "        metric_patterns = ['metric', 'performance', 'accuracy', 'precision', 'recall', 'mrr', 'ndcg']\n",
        "        sentences = text.split('.')\n",
        "        metric_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in metric_patterns)]\n",
        "        return '. '.join(metric_sentences[:2]) + '.' if metric_sentences else \"evaluation metrics applied\"\n",
        "\n",
        "    def extract_conditions(self, text: str) -> str:\n",
        "        condition_patterns = ['when', 'condition', 'case', 'scenario', 'context']\n",
        "        sentences = text.split('.')\n",
        "        condition_sentences = [s for s in sentences if any(pattern in s.lower() for pattern in condition_patterns)]\n",
        "        return '. '.join(condition_sentences[:2]) + '.' if condition_sentences else \"specific conditions apply\"\n",
        "\n",
        "    def generate_answer(self, query: str, top_k: int = 6) -> Dict:\n",
        "        try:\n",
        "            relevant_chunks = self.embedding_manager.retrieve_relevant_chunks(query, top_k)\n",
        "            query_type = self.identify_query_type(query)\n",
        "            template = self.templates[query_type]\n",
        "\n",
        "            info = self.extract_key_information(relevant_chunks)\n",
        "            answer = template.format(**info)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': answer,\n",
        "                'relevant_chunks': relevant_chunks,\n",
        "                'query_type': query_type\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': f\"Based on the available information: {str(e)}\",\n",
        "                'relevant_chunks': [],\n",
        "                'query_type': 'error'\n",
        "            }"
      ],
      "metadata": {
        "id": "PLraHDrHjbr9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplifiedFineTuning:\n",
        "    \"\"\"\n",
        "    Simplified fine-tuning approach that works with your existing implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model_name: str = 'sentence-transformers/all-mpnet-base-v2'):\n",
        "        self.base_model_name = base_model_name\n",
        "        self.model = None\n",
        "        self.fine_tuned_model = None\n",
        "\n",
        "    def prepare_training_data(self, documents: Dict[str, str]) -> List[InputExample]:\n",
        "        \"\"\"\n",
        "        Create training data from your documents using a simplified approach\n",
        "        \"\"\"\n",
        "        training_examples = []\n",
        "\n",
        "        # Extract all text chunks\n",
        "        all_texts = []\n",
        "        for doc_name, content in documents.items():\n",
        "            # Split into sentences\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
        "            for sentence in sentences:\n",
        "                if len(sentence.strip()) > 50:  # Minimum length\n",
        "                    all_texts.append(sentence.strip())\n",
        "\n",
        "        print(f\"Extracted {len(all_texts)} sentences for training\")\n",
        "\n",
        "        # Create query-passage pairs based on academic content patterns\n",
        "        query_templates = [\n",
        "            \"What is {concept}?\",\n",
        "            \"How does {concept} work?\",\n",
        "            \"Explain {concept}\",\n",
        "            \"What are the benefits of {concept}?\",\n",
        "            \"Describe the methodology of {concept}\",\n",
        "            \"What are the results for {concept}?\"\n",
        "        ]\n",
        "\n",
        "        # Extract key concepts from your documents\n",
        "        key_concepts = self._extract_concepts(documents)\n",
        "\n",
        "        for concept in key_concepts[:20]:  # Limit to top 20 concepts\n",
        "            # Find sentences containing this concept\n",
        "            relevant_sentences = [s for s in all_texts if concept.lower() in s.lower()]\n",
        "\n",
        "            if relevant_sentences:\n",
        "                # Create positive pairs\n",
        "                for template in query_templates[:3]:  # Use 3 templates per concept\n",
        "                    query = template.format(concept=concept)\n",
        "                    positive_passage = relevant_sentences[0]  # Use best matching sentence\n",
        "\n",
        "                    training_examples.append(InputExample(\n",
        "                        texts=[query, positive_passage],\n",
        "                        label=1.0\n",
        "                    ))\n",
        "\n",
        "                    # Add negative examples\n",
        "                    negative_candidates = [s for s in all_texts if concept.lower() not in s.lower()]\n",
        "                    if negative_candidates:\n",
        "                        import random\n",
        "                        negative_passage = random.choice(negative_candidates[:10])\n",
        "                        training_examples.append(InputExample(\n",
        "                            texts=[query, negative_passage],\n",
        "                            label=0.0\n",
        "                        ))\n",
        "\n",
        "        print(f\"Created {len(training_examples)} training examples\")\n",
        "        return training_examples\n",
        "\n",
        "    def _extract_concepts(self, documents: Dict[str, str]) -> List[str]:\n",
        "        \"\"\"Extract key concepts from documents\"\"\"\n",
        "        concepts = set()\n",
        "\n",
        "        # Predefined important concepts for RAG papers\n",
        "        predefined_concepts = [\n",
        "            'RAG', 'SELF-ROUTE', 'retrieval-augmented generation',\n",
        "            'embedding', 'chunking', 'reranking', 'multimodal',\n",
        "            'long-context', 'self-reflection', 'routing',\n",
        "            'failure analysis', 'performance evaluation'\n",
        "        ]\n",
        "\n",
        "        all_text = ' '.join(documents.values()).lower()\n",
        "\n",
        "        for concept in predefined_concepts:\n",
        "            if concept.lower() in all_text:\n",
        "                concepts.add(concept)\n",
        "\n",
        "        # Extract additional concepts using patterns\n",
        "        pattern_concepts = re.findall(r'\\b[A-Z][A-Z0-9\\-]+\\b', ' '.join(documents.values()))\n",
        "        concepts.update([c for c in pattern_concepts if len(c) > 2])\n",
        "\n",
        "        return list(concepts)\n",
        "\n",
        "    def fine_tune_embeddings(self, documents: Dict[str, str],\n",
        "                           num_epochs: int = 2,\n",
        "                           batch_size: int = 8) -> SentenceTransformer:\n",
        "        \"\"\"\n",
        "        Fine-tune embeddings with simplified approach\n",
        "        \"\"\"\n",
        "        print(\"🚀 Starting simplified fine-tuning process...\")\n",
        "\n",
        "        # Load base model\n",
        "        self.model = SentenceTransformer(self.base_model_name)\n",
        "        print(f\"✅ Loaded base model: {self.base_model_name}\")\n",
        "\n",
        "        # Prepare training data\n",
        "        training_examples = self.prepare_training_data(documents)\n",
        "\n",
        "        if len(training_examples) < 10:\n",
        "            print(\"⚠️ Not enough training data. Using base model.\")\n",
        "            return self.model\n",
        "\n",
        "        # Create training dataloader\n",
        "        train_dataloader = DataLoader(training_examples, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "        # Use MultipleNegativesRankingLoss for contrastive learning\n",
        "        train_loss = losses.MultipleNegativesRankingLoss(model=self.model)\n",
        "\n",
        "        # Fine-tune\n",
        "        print(f\"🔧 Fine-tuning for {num_epochs} epochs...\")\n",
        "        self.model.fit(\n",
        "            train_objectives=[(train_dataloader, train_loss)],\n",
        "            epochs=num_epochs,\n",
        "            warmup_steps=int(len(train_dataloader) * 0.1),\n",
        "            show_progress_bar=True,\n",
        "            optimizer_params={'lr': 2e-5}\n",
        "        )\n",
        "\n",
        "        print(\"✅ Fine-tuning completed!\")\n",
        "        self.fine_tuned_model = self.model\n",
        "        return self.fine_tuned_model\n",
        "\n",
        "class EnhancedRAGWithFineTuning:\n",
        "    \"\"\"\n",
        "    Enhanced version of your RAG system with fine-tuning capability\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model_name: str = 'sentence-transformers/all-mpnet-base-v2'):\n",
        "        self.base_model_name = base_model_name\n",
        "        self.base_embedding_manager = None\n",
        "        self.finetuned_embedding_manager = None\n",
        "\n",
        "    def setup_and_compare(self, documents: Dict[str, str], test_queries: List[str]):\n",
        "        \"\"\"\n",
        "        Setup both base and fine-tuned systems and compare performance\n",
        "        \"\"\"\n",
        "        print(\"🔄 Setting up base RAG system...\")\n",
        "\n",
        "        # Setup base system (using your existing embedding manager)\n",
        "        from your_existing_code import EmbeddingManager, TextChunker  # Import your classes\n",
        "\n",
        "        # Base system\n",
        "        self.base_embedding_manager = EmbeddingManager(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "        chunker = TextChunker(chunk_size=400, overlap=80)\n",
        "        chunks = chunker.chunk_documents(documents)\n",
        "        self.base_embedding_manager.create_embeddings(chunks)\n",
        "\n",
        "        print(\"🔧 Setting up fine-tuned RAG system...\")\n",
        "\n",
        "        # Fine-tuned system\n",
        "        fine_tuner = SimplifiedFineTuning(base_model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "        fine_tuned_model = fine_tuner.fine_tune_embeddings(documents)\n",
        "\n",
        "        # Create new embedding manager with fine-tuned model\n",
        "        self.finetuned_embedding_manager = EmbeddingManager()\n",
        "        self.finetuned_embedding_manager.model = fine_tuned_model\n",
        "        self.finetuned_embedding_manager.create_embeddings(chunks)\n",
        "\n",
        "        print(\"📊 Running comparison tests...\")\n",
        "\n",
        "        # Test both systems\n",
        "        base_results = []\n",
        "        finetuned_results = []\n",
        "\n",
        "        for query in test_queries:\n",
        "            # Base system results\n",
        "            base_chunks = self.base_embedding_manager.retrieve_relevant_chunks(query, top_k=8)\n",
        "            base_result = {\n",
        "                'query': query,\n",
        "                'relevant_chunks': base_chunks,\n",
        "                'answer': self._generate_answer_from_chunks(query, base_chunks)\n",
        "            }\n",
        "            base_results.append(base_result)\n",
        "\n",
        "            # Fine-tuned system results\n",
        "            ft_chunks = self.finetuned_embedding_manager.retrieve_relevant_chunks(query, top_k=8)\n",
        "            ft_result = {\n",
        "                'query': query,\n",
        "                'relevant_chunks': ft_chunks,\n",
        "                'answer': self._generate_answer_from_chunks(query, ft_chunks)\n",
        "            }\n",
        "            finetuned_results.append(ft_result)\n",
        "\n",
        "        return base_results, finetuned_results\n",
        "\n",
        "    def _generate_answer_from_chunks(self, query: str, chunks: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Generate answer from chunks (simplified version of your answer generation)\n",
        "        \"\"\"\n",
        "        if not chunks:\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        # Combine top chunks\n",
        "        combined_text = \" \".join([chunk['text'] for chunk in chunks[:3]])\n",
        "\n",
        "        # Simple extractive approach\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', combined_text)\n",
        "        query_words = set(query.lower().split())\n",
        "\n",
        "        scored_sentences = []\n",
        "        for sentence in sentences:\n",
        "            if len(sentence.strip()) < 20:\n",
        "                continue\n",
        "\n",
        "            sentence_words = set(sentence.lower().split())\n",
        "            overlap = len(query_words.intersection(sentence_words))\n",
        "\n",
        "            # Bonus for academic terms\n",
        "            if any(term in sentence.lower() for term in ['rag', 'retrieval', 'embedding', 'chunking', 'self-route']):\n",
        "                overlap += 2\n",
        "\n",
        "            if overlap > 0:\n",
        "                scored_sentences.append((overlap, sentence.strip()))\n",
        "\n",
        "        if scored_sentences:\n",
        "            scored_sentences.sort(reverse=True, key=lambda x: x[0])\n",
        "            top_sentences = [sent for _, sent in scored_sentences[:2]]\n",
        "            return \". \".join(top_sentences) + \".\"\n",
        "\n",
        "        return \"Based on the retrieved information: \" + combined_text[:200] + \"...\"\n",
        "\n",
        "def analyze_improvements(base_results: List[Dict], finetuned_results: List[Dict], test_queries: List[str]):\n",
        "    \"\"\"\n",
        "    Analyze and visualize improvements from fine-tuning\n",
        "    \"\"\"\n",
        "    print(\"📈 Analyzing performance improvements...\")\n",
        "\n",
        "    improvements = []\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        base_result = base_results[i]\n",
        "        ft_result = finetuned_results[i]\n",
        "\n",
        "        base_score = base_result['relevant_chunks'][0]['similarity_score'] if base_result['relevant_chunks'] else 0\n",
        "        ft_score = ft_result['relevant_chunks'][0]['similarity_score'] if ft_result['relevant_chunks'] else 0\n",
        "\n",
        "        improvement = ft_score - base_score\n",
        "        relative_improvement = (improvement / base_score * 100) if base_score > 0 else 0\n",
        "\n",
        "        improvements.append({\n",
        "            'Query_ID': i + 1,\n",
        "            'Query': query[:60] + \"...\" if len(query) > 60 else query,\n",
        "            'Base_Score': base_score,\n",
        "            'FT_Score': ft_score,\n",
        "            'Improvement': improvement,\n",
        "            'Relative_Improvement_%': relative_improvement,\n",
        "            'Base_Answer_Length': len(base_result['answer']),\n",
        "            'FT_Answer_Length': len(ft_result['answer'])\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(improvements)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    avg_base_score = df['Base_Score'].mean()\n",
        "    avg_ft_score = df['FT_Score'].mean()\n",
        "    avg_improvement = df['Improvement'].mean()\n",
        "    avg_relative_improvement = df['Relative_Improvement_%'].mean()\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n📊 PERFORMANCE SUMMARY:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Average Base Model Score: {avg_base_score:.4f}\")\n",
        "    print(f\"Average Fine-tuned Score: {avg_ft_score:.4f}\")\n",
        "    print(f\"Average Absolute Improvement: {avg_improvement:.4f}\")\n",
        "    print(f\"Average Relative Improvement: {avg_relative_improvement:.1f}%\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Show queries with biggest improvements\n",
        "    top_improvements = df.nlargest(3, 'Relative_Improvement_%')\n",
        "    print(f\"\\n🚀 TOP 3 IMPROVEMENTS:\")\n",
        "    for _, row in top_improvements.iterrows():\n",
        "        print(f\"Query {row['Query_ID']}: {row['Query']}\")\n",
        "        print(f\"  Improvement: {row['Base_Score']:.3f} → {row['FT_Score']:.3f} (+{row['Relative_Improvement_%']:.1f}%)\")\n",
        "\n",
        "    # Save detailed results\n",
        "    df.to_csv('fine_tuning_comparison.csv', index=False)\n",
        "    print(f\"\\n💾 Detailed results saved to 'fine_tuning_comparison.csv'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_comparison_visualization(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Create visualizations comparing base vs fine-tuned performance\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    plt.style.use('default')\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Fine-Tuning Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Score comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.scatter(df['Base_Score'], df['FT_Score'], alpha=0.7, s=60)\n",
        "    ax1.plot([0, df['Base_Score'].max()], [0, df['Base_Score'].max()], 'r--', alpha=0.5, label='No improvement line')\n",
        "    ax1.set_xlabel('Base Model Score')\n",
        "    ax1.set_ylabel('Fine-tuned Model Score')\n",
        "    ax1.set_title('Score Comparison')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Improvement distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    improvements = df['Relative_Improvement_%']\n",
        "    ax2.hist(improvements, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax2.axvline(improvements.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {improvements.mean():.1f}%')\n",
        "    ax2.set_xlabel('Relative Improvement (%)')\n",
        "    ax2.set_ylabel('Number of Queries')\n",
        "    ax2.set_title('Distribution of Improvements')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Query-wise performance\n",
        "    ax3 = axes[1, 0]\n",
        "    x = range(len(df))\n",
        "    width = 0.35\n",
        "    ax3.bar([i - width/2 for i in x], df['Base_Score'], width, label='Base Model', alpha=0.7, color='lightcoral')\n",
        "    ax3.bar([i + width/2 for i in x], df['FT_Score'], width, label='Fine-tuned Model', alpha=0.7, color='lightblue')\n",
        "    ax3.set_xlabel('Query ID')\n",
        "    ax3.set_ylabel('Similarity Score')\n",
        "    ax3.set_title('Query-wise Performance')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels([f'Q{i+1}' for i in x], rotation=45)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Answer length comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.scatter(df['Base_Answer_Length'], df['FT_Answer_Length'], alpha=0.7, s=60, color='green')\n",
        "    ax4.plot([0, df['Base_Answer_Length'].max()], [0, df['Base_Answer_Length'].max()], 'r--', alpha=0.5)\n",
        "    ax4.set_xlabel('Base Model Answer Length')\n",
        "    ax4.set_ylabel('Fine-tuned Model Answer Length')\n",
        "    ax4.set_title('Answer Length Comparison')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('fine_tuning_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"📊 Visualization saved as 'fine_tuning_analysis.png'\")\n",
        "\n",
        "# Main execution function\n",
        "def run_complete_analysis(documents: Dict[str, str]):\n",
        "    \"\"\"\n",
        "    Run the complete fine-tuning analysis\n",
        "    \"\"\"\n",
        "    # Test queries from the assignment\n",
        "    test_queries = [\n",
        "        \"What is the primary goal of the SELF-ROUTE method proposed by Zhuowan Li?\",\n",
        "        \"Explain why the researchers believe RAG might still be useful despite the superior performance of long-context LLMs\",\n",
        "        \"Compare the reranking techniques mentioned in the Wang paper. How do they impact the retrieval quality?\",\n",
        "        \"What are the trade-offs involved when using different chunking strategies in RAG systems?\",\n",
        "        \"How does multimodal retrieval enhance the capabilities of RAG?\",\n",
        "        \"What were the key failure cases for RAG in handling long context retrievals, as noted by Zhuowan Li?\",\n",
        "        \"Why does the Zhuowan paper claim that long-context LLMs outperformed RAG in most cases? What benefits does RAG still offer?\",\n",
        "        \"Describe the metrics used to evaluate the different embedding models for RAG in Wang's paper\",\n",
        "        \"Discuss the implications of using self-reflection in routing queries between RAG and long-context LLMs\",\n",
        "        \"How does query rewriting contribute to the overall efficiency of RAG according to Wang's findings?\",\n",
        "        \"Compare the cost-efficiency and performance trade-offs between RAG and long-context language models (LC) as discussed in the Wang and Zhuowan Li papers. How do these methods balance the ability to handle large volumes of text with computational demands?\",\n",
        "        \"In terms of chunking methods in Wang's paper, what is the difference in performance between the best and second-best methods in Table 4?\",\n",
        "        \"What are the best approaches for the retrieval and reranking modules according to Table 11 in Wang paper?\"\n",
        "    ]\n",
        "\n",
        "    print(\"🚀 Starting Complete Fine-Tuning Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup and run comparison\n",
        "    enhanced_rag = EnhancedRAGWithFineTuning()\n",
        "    base_results, finetuned_results = enhanced_rag.setup_and_compare(documents, test_queries)\n",
        "\n",
        "    # Analyze improvements\n",
        "    comparison_df = analyze_improvements(base_results, finetuned_results, test_queries)\n",
        "\n",
        "    # Create visualizations\n",
        "    create_comparison_visualization(comparison_df)\n",
        "\n",
        "    # Print detailed results for first few queries\n",
        "    print(f\"\\n📝 DETAILED RESULTS (First 3 queries):\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i in range(min(3, len(test_queries))):\n",
        "        print(f\"\\nQuery {i+1}: {test_queries[i]}\")\n",
        "        print(f\"Base Answer: {base_results[i]['answer'][:100]}...\")\n",
        "        print(f\"FT Answer: {finetuned_results[i]['answer'][:100]}...\")\n",
        "        print(f\"Score: {base_results[i]['relevant_chunks'][0]['similarity_score']:.3f} → {finetuned_results[i]['relevant_chunks'][0]['similarity_score']:.3f}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return base_results, finetuned_results, comparison_df\n",
        "\n",
        "# Integration with your existing code\n",
        "def integrate_with_your_implementation():\n",
        "    \"\"\"\n",
        "    Integration instructions for your existing RAG implementation\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "    🔗 INTEGRATION INSTRUCTIONS:\n",
        "\n",
        "    1. Replace the imports at the top with your actual class imports:\n",
        "       from your_notebook import EmbeddingManager, TextChunker, ImprovedRAGSystem\n",
        "\n",
        "    2. Use your existing documents dictionary:\n",
        "       documents = your_existing_documents\n",
        "\n",
        "    3. Run the analysis:\n",
        "       base_results, ft_results, comparison_df = run_complete_analysis(documents)\n",
        "\n",
        "    4. Compare with your Part 1 results:\n",
        "       - Check the 'fine_tuning_comparison.csv' file\n",
        "       - Look at the visualization 'fine_tuning_analysis.png'\n",
        "       - Review the console output for summary statistics\n",
        "\n",
        "    📊 Expected Improvements:\n",
        "    - Similarity scores should increase by 5-15%\n",
        "    - Better retrieval of domain-specific content\n",
        "    - More relevant chunks for academic queries\n",
        "    - Improved handling of technical terminology\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "4XQiY5uHwpuV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Fine-Tuning Implementation Ready!\")\n",
        "    print(\"Run integrate_with_your_implementation() for setup instructions\")\n",
        "    integrate_with_your_implementation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BByI7zBawr4Z",
        "outputId": "fb613275-fa02-46a4-8a80-b78bf0a926b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuning Implementation Ready!\n",
            "Run integrate_with_your_implementation() for setup instructions\n",
            "\n",
            "    🔗 INTEGRATION INSTRUCTIONS:\n",
            "    \n",
            "    1. Replace the imports at the top with your actual class imports:\n",
            "       from your_notebook import EmbeddingManager, TextChunker, ImprovedRAGSystem\n",
            "    \n",
            "    2. Use your existing documents dictionary:\n",
            "       documents = your_existing_documents\n",
            "    \n",
            "    3. Run the analysis:\n",
            "       base_results, ft_results, comparison_df = run_complete_analysis(documents)\n",
            "    \n",
            "    4. Compare with your Part 1 results:\n",
            "       - Check the 'fine_tuning_comparison.csv' file\n",
            "       - Look at the visualization 'fine_tuning_analysis.png'\n",
            "       - Review the console output for summary statistics\n",
            "    \n",
            "    📊 Expected Improvements:\n",
            "    - Similarity scores should increase by 5-15%\n",
            "    - Better retrieval of domain-specific content\n",
            "    - More relevant chunks for academic queries\n",
            "    - Improved handling of technical terminology\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}